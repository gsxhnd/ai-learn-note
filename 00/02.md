## 零基础入门深度学习(2) - 线性单元和梯度下降

原文链接: <https://www.zybuluo.com/hanbingtao/note/448086>

### 往期回顾

在上一篇文章中，我们已经学会了编写一个简单的感知器，并用它来实现一个线性分类器。你应该还记得用来训练感知器的『感知器规则』。然而，我们并没有关心这个规则是怎么得到的。本文通过介绍另外一种『感知器』，也就是『线性单元』，来说明关于机器学习一些基本的概念，比如模型、目标函数、优化算法等等。这些概念对于所有的机器学习算法来说都是通用的，掌握了这些概念，就掌握了机器学习的基本套路。

### 线性单元是啥

感知器有一个问题，当面对的数据集不是**线性可分**的时候，『感知器规则』可能无法收敛，这意味着我们永远也无法完成一个感知器的训练。为了解决这个问题，我们使用一个**可导**的**线性函数**来替代感知器的**阶跃函数**，这种感知器就叫做**线性单元**。线性单元在面对线性不可分的数据集时，会收敛到一个最佳的近似上。

为了简单起见，我们可以设置线性单元的激活函数$$f$$为

$$
f(x) = x
$$

这样的线性单元如下图所示

![02-01](../.gitbook/assets/00/02-01.png)

对比此前我们讲过的感知器

![01-02](../.gitbook/assets/00/01-02.png)

这样替换了激活函数之后，**线性单元**将返回一个**实数值**而不是**0,1 分类**。因此线性单元用来解决**回归**问题而不是**分类**问题。

#### 线性单元的模型

当我们说**模型**时，我们实际上在谈论根据输入$$x$$预测输出$$y$$的**算法**。比如，$$x$$可以是一个人的工作年限，$$y$$可以是他的月薪，我们可以用某种算法来根据一个人的工作年限来预测他的收入。比如：

$$
y=h(x)=w*x+b
$$

函数$$h(x)$$叫做**假设**，而$$w$$、$$b$$是它的**参数**。我们假设参数$$w=1000$$，参数$$b=500$$，如果一个人的工作年限是 5 年的话，我们的模型会预测他的月薪为

$$
y=h(x)=1000*5+500=5500(元)
$$

你也许会说，这个模型太不靠谱了。是这样的，因为我们考虑的因素太少了，仅仅包含了工作年限。如果考虑更多的因素，比如所处的行业、公司、职级等等，可能预测就会靠谱的多。我们把工作年限、行业、公司、职级这些信息，称之为**特征**。对于一个工作了 5 年，在 IT 行业，百度工作，职级 T6 这样的人，我们可以用这样的一个特征向量来表示他

$$\mathrm{x}$$ = _(5, IT, 百度, T6)_。

既然输入$$\mathrm{x}$$变成了一个具备四个特征的向量，相对应的，仅仅一个参数$$w$$就不够用了，我们应该使用 4 个参数$$w_1,w_2,w_3,w_4$$，每个特征对应一个。这样，我们的模型就变成

$$
y=h(x)=w_1*x_1+w_2*x_2+w_3*x_3+w_4*x_4+b
$$

其中，$$x_1$$对应工作年限，$$x_2$$对应行业，$$x_3$$对应公司，$$x_4$$对应职级。

为了书写和计算方便，我们可以令$$w_0$$等于$$b$$，同时令$$w_0$$对应于特征$$x_0$$。由于$$x_0$$其实并不存在，我们可以令它的值永远为 1。也就是说

$$
b = w_0 * x_0\qquad其中x_0=1
$$

这样上面的式子就可以写成

$$
\begin{align}
y=h(x)&=w_1*x_1+w_2*x_2+w_3*x_3+w_4*x_4+b\\
&=w_0*x_0+w_1*x_1+w_2*x_2+w_3*x_3+w_4*x_4
\end{align}
$$

我们还可以把上式写成向量的形式

$$
y=h(x)=\mathrm{w}^T\mathrm{x}\qquad\qquad(式1)
$$

长成这种样子模型就叫做**线性模型**，因为输出就是输入特征的**线性组合**。

#### 监督学习和无监督学习

接下来，我们需要关心的是这个模型如何训练，也就是参数取什么值最合适。

机器学习有一类学习方法叫做**监督学习**，它是说为了训练一个模型，我们要提供这样一堆训练样本：每个训练样本既包括输入特征$\mathrm{x}$，也包括对应的输出$$y$$($$y$$也叫做**标记，label**)。也就是说，我们要找到很多人，我们既知道他们的特征(工作年限，行业...)，也知道他们的收入。我们用这样的样本去训练模型，让模型既看到我们提出的每个问题(输入特征$$\mathrm{x}$$)，也看到对应问题的答案(标记$$y$$)。当模型看到足够多的样本之后，它就能总结出其中的一些规律。然后，就可以预测那些它没看过的输入所对应的答案了。

另外一类学习方法叫做**无监督学习**，这种方法的训练样本中只有$$\mathrm{x}$$而没有$$y$$。模型可以总结出特征$$\mathrm{x}$$的一些规律，但是无法知道其对应的答案$$y$$。

很多时候，既有$$\mathrm{x}$$又有$$y$$的训练样本是很少的，大部分样本都只有$$\mathrm{x}$$。比如在语音到文本(STT)的识别任务中，$$\mathrm{x}$$是语音，$$y$$是这段语音对应的文本。我们很容易获取大量的语音录音，然而把语音一段一段切分好并**标注**上对应文字则是非常费力气的事情。这种情况下，为了弥补带标注样本的不足，我们可以用**无监督学习方法**先做一些**聚类**，让模型总结出哪些音节是相似的，然后再用少量的带标注的训练样本，告诉模型其中一些音节对应的文字。这样模型就可以把相似的音节都对应到相应文字上，完成模型的训练。

#### 线性单元的目标函数

现在，让我们只考虑**监督学习**。

在监督学习下，对于一个样本，我们知道它的特征$$\mathrm{x}$$，以及标记$$y$$。同时，我们还可以根据模型$$h(x)$$计算得到输出$$\bar{y}$$。注意这里面我们用$$y$$表示训练样本里面的**标记**，也就是**实际值**；用带上划线的$$\bar{y}$$表示模型计算的出来的**预测值**。我们当然希望模型计算出来的$$\bar{y}$$和$$y$$越接近越好。

数学上有很多方法来表示的$$\bar{y}$$和$$y$$的接近程度，比如我们可以用$$\bar{y}$$和$$y$$的差的平方的$$\frac{1}{2}$$来表示它们的接近程度

$$
e=\frac{1}{2}(y-\bar{y})^2
$$

我们把$$e$$叫做**单个样本**的**误差**。至于为什么前面要乘$$\frac{1}{2}$$，是为了后面计算方便。

训练数据中会有很多样本，比如$$N$$个，我们可以用训练数据中**所有样本**的误差的**和**，来表示模型的误差$$E$$，也就是

$$
E=e^{(1)}+e^{(2)}+e^{(3)}+...+e^{(n)}
$$

上式的$$e^{(1)}$$表示第一个样本的误差，$$e^{(2)}$$表示第二个样本的误差......。

我们还可以把上面的式子写成和式的形式。使用和式，不光书写起来简单，逼格也跟着暴涨，一举两得。所以一定要写成下面这样

$$
\begin{align}
E&=e^{(1)}+e^{(2)}+e^{(3)}+...+e^{(n)}\\
&=\sum_{i=1}^{n}e^{(i)}\\
&=\frac{1}{2}\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})^2\qquad\qquad(式2)
\end{align}
$$

其中

$$
\begin{align}
\bar{y}^{(i)}&=h(\mathrm{x}^{(i)})\\
&=\mathrm{w}^T\mathrm{x^{(i)}}
\end{align}
$$

(式 2)中，$$x^{(i)}$$表示第$$i$$个训练样本的**特征**，$$y^{(i)}$$表示第$$i$$个样本的**标记**，我们也可以用**元组**$$(x^{(i)},y^{(i)})$$表示第$$i$$**训练样本**。$$\bar{y}^{(i)}$$则是模型对第个$$i$$样本的**预测值**。

我们当然希望对于一个训练数据集来说，误差最小越好，也就是(式 2)的值越小越好。对于特定的训练数据集来说，$$(x^{(i)},y^{(i)})$$的值都是已知的，所以(式 2)其实是参数$$w$$的函数。

$$
\begin{align}
E(\mathrm{w})&=\frac{1}{2}\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})^2\\
&=\frac{1}{2}\sum_{i=1}^{n}(\mathrm{y^{(i)}}-\mathrm{w}^Tx^{(i)})^2
\end{align}
$$

由此可见，模型的训练，实际上就是求取到合适的$$w$$，使(式 2)取得最小值。这在数学上称作**优化问题**，而$$E(\mathrm{w})$$就是我们优化的目标，称之为**目标函数**。

#### 梯度下降优化算法

大学时我们学过怎样求函数$$y=f(x)$$的极值。函数的极值点，就是它的导数$$f'(x)=0$$的那个点。因此我们可以通过解方程$$f'(x)=0$$，求得函数的极值点$$(x_0,y_0)$$。

不过对于计算机来说，它可不会解方程。但是它可以凭借强大的计算能力，一步一步的去把函数的极值点『试』出来。如下图所示：

![02-02](../.gitbook/assets/00/02-02.png)

首先，我们随便选择一个点开始，比如上图的$$x_0$$点。接下来，每次迭代修改$$x$$的为$$x_1,x_2,x_3,...$$，经过数次迭代后最终达到函数最小值点。

你可能要问了，为啥每次修改$$x$$的值，都能往函数最小值那个方向前进呢？这里的奥秘在于，我们每次都是向函数$$y=f(x)$$的**梯度**的**相反方向**来修改。什么是**梯度**呢？翻开大学高数课的课本，我们会发现**梯度**是一个向量，它指向**函数值上升最快**的方向。显然，梯度的反方向当然就是函数值下降最快的方向了。我们每次沿着梯度相反方向去修改的值，当然就能走到函数的最小值附近。之所以是最小值附近而不是最小值那个点，是因为我们每次移动的步长不会那么恰到好处，有可能最后一次迭代走远了越过了最小值那个点。步长的选择是门手艺，如果选择小了，那么就会迭代很多轮才能走到最小值附近；如果选择大了，那可能就会越过最小值很远，收敛不到一个好的点上。

按照上面的讨论，我们就可以写出梯度下降算法的公式

$$
\mathrm{x}_{new}=\mathrm{x}_{old}-\eta\nabla{f(x)}
$$

其中，$$\nabla$$是**梯度算子**，$$\nabla{f(x)}$$就是指$$f(x)$$的梯度。$$\eta$$是步长，也称作**学习速率**。

对于上一节列出的目标函数(式 2)

$$
E(\mathrm{w})=\frac{1}{2}\sum_{i=1}^{n}(\mathrm{y^{(i)}-\bar{y}^{(i)}})^2
$$

梯度下降算法可以写成

$$
\mathrm{w}_{new}=\mathrm{w}_{old}-\eta\nabla{E(\mathrm{w})}
$$

聪明的你应该能想到，如果要求目标函数的**最大值**，那么我们就应该用**梯度上升**算法，它的参数修改规则是

$$
\mathrm{w}_{new}=\mathrm{w}_{old}+\eta\nabla{E(\mathrm{w})}
$$

下面，请先做几次深呼吸，让你的大脑补充足够的新鲜的氧气，**我们要来求取**$$\nabla{E(\mathrm{w})}$$，然后带入上式，就能得到线性单元的参数修改规则。

关于$$\nabla{E(\mathrm{w})}$$的推导过程，我单独把它们放到一节中。您既可以选择慢慢看，也可以选择无视。在这里，您只需要知道，经过一大串推导，目标函数$$E(w)$$的梯度是

$$
\nabla{E(\mathrm{w})}=-\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})\mathrm{x}^{(i)}
$$

因此，线性单元的参数修改规则最后是这个样子

$$
\mathrm{w}_{new}=\mathrm{w}_{old}+\eta\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})\mathrm{x}^{(i)}\qquad\qquad(式3)
$$

有了上面这个式子，我们就可以根据它来写出训练线性单元的代码了。

需要说明的是，如果每个样本有 M 个特征，则上式中的$\mathrm{x},\mathrm{w}$都是 M+1 维**向量**(因为我们加上了一个恒为 1 的虚拟特征，参考前面的内容)，而是**标量**。用高逼格的数学符号表示，就是

##### $$\nabla{E}(\mathrm{w})$$的推导

#### 随机梯度下降算法(Stochastic Gradient Descent, SGD)

### 实现线性单元

### 小结
