# 长短时记忆网络(LSTM)

## 往期回顾

在上一篇文章中，我们介绍了**循环神经网络**以及它的训练算法。我们也介绍了**循环神经网络**很难训练的原因，这导致了它在实际应用中，很难处理长距离的依赖。在本文中，我们将介绍一种改进之后的循环神经网络：**长短时记忆网络(Long Short Term Memory Network, LSTM)**，它成功的解决了原始循环神经网络的缺陷，成为当前最流行的RNN，在语音识别、图片描述、自然语言处理等许多领域中成功应用。但不幸的一面是，**LSTM**的结构很复杂，因此，我们需要花上一些力气，才能把LSTM以及它的训练算法弄明白。在搞清楚**LSTM**之后，我们再介绍一种**LSTM**的变体：**GRU (Gated Recurrent Unit)**。 它的结构比**LSTM**简单，而效果却和**LSTM**一样好，因此，它正在逐渐流行起来。最后，我们仍然会动手实现一个**LSTM**。

## 长短时记忆网络是啥

我们首先了解一下长短时记忆网络产生的背景。回顾一下[零基础入门深度学习(5) - 循环神经网络](https://zybuluo.com/hanbingtao/note/541458)中推导的，误差项沿时间反向传播的公式：


$$
\begin{align}
\delta_k^T=&\delta_t^T\prod_{i=k}^{t-1}diag[f'(\mathbf{net}_{i})]W\\
\end{align}
$$




我们可以根据下面的不等式，来获取的模的上界（模可以看做对中每一项值的大小的度量）：
$$
\begin{align}
\|\delta_k^T\|\leqslant&\|\delta_t^T\|\prod_{i=k}^{t-1}\|diag[f'(\mathbf{net}_{i})]\|\|W\|\\
\leqslant&\|\delta_t^T\|(\beta_f\beta_W)^{t-k}
\end{align}
$$






我们可以看到，误差项$$\delta$$从t时刻传递到k时刻，其值的上界是$$\beta_f\beta_w$$的指数函数。$$\beta_f\beta_w$$分别是对角矩阵$$diag[f'(\mathbf{net}_{i})]$$和矩阵W模的上界。显然，除非$$\beta_f\beta_w$$乘积的值位于1附近，否则，当t-k很大时（也就是误差传递很多个时刻时），整个式子的值就会变得极小（当$$\beta_f\beta_w$$乘积小于1）或者极大（当$$\beta_f\beta_w$$乘积大于1），前者就是**梯度消失**，后者就是**梯度爆炸**。虽然科学家们搞出了很多技巧（比如怎样初始化权重），让$$\beta_f\beta_w$$的值尽可能贴近于1，终究还是难以抵挡指数函数的威力。

**梯度消失**到底意味着什么？在[零基础入门深度学习(5) - 循环神经网络](https://zybuluo.com/hanbingtao/note/541458)中我们已证明，权重数组W最终的梯度是各个时刻的梯度之和，即：
$$
\begin{align}
\nabla_WE&=\sum_{k=1}^t\nabla_{Wk}E\\
&=\nabla_{Wt}E+\nabla_{Wt-1}E+\nabla_{Wt-2}E+...+\nabla_{W1}E
\end{align}
$$






假设某轮训练中，各时刻的梯度以及最终的梯度之和如下图：

![img](http://upload-images.jianshu.io/upload_images/2256672-48784f6366412472.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们就可以看到，从上图的t-3时刻开始，梯度已经几乎减少到0了。那么，从这个时刻开始再往之前走，得到的梯度（几乎为零）就不会对最终的梯度值有任何贡献，这就相当于无论t-3时刻之前的网络状态h是什么，在训练中都不会对权重数组W的更新产生影响，也就是网络事实上已经忽略了t-3时刻之前的状态。这就是原始RNN无法处理长距离依赖的原因。

既然找到了问题的原因，那么我们就能解决它。从问题的定位到解决，科学家们大概花了7、8年时间。终于有一天，Hochreiter和Schmidhuber两位科学家发明出**长短时记忆网络**，一举解决这个问题。

其实，**长短时记忆网络**的思路比较简单。原始RNN的隐藏层只有一个状态，即h，它对于短期的输入非常敏感。那么，假如我们再增加一个状态，即c，让它来保存长期的状态，那么问题不就解决了么？如下图所示：

![img](http://upload-images.jianshu.io/upload_images/2256672-71de4194da5a5ec4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

新增加的状态c，称为**单元状态(cell state)**。我们把上图按照时间维度展开：

![img](http://upload-images.jianshu.io/upload_images/2256672-715658c134b9d6f1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图仅仅是一个示意图，我们可以看出，在t时刻，LSTM的输入有三个：当前时刻网络的输入值$$\mathbf{x}_t$$、上一时刻LSTM的输出值$$\mathbf{h}_{t-1}$$、以及上一时刻的单元状态$$\mathbf{c}_{t-1}$$；LSTM的输出有两个：当前时刻LSTM输出值$$\mathbf{h}_t$$、和当前时刻的单元状态$$\mathbf{c}_t$$。注意$$\mathbf{x}$$、$$\mathbf{h}$$、$$\mathbf{c}$$都是**向量**。

LSTM的关键，就是怎样控制长期状态c。在这里，LSTM的思路是使用三个控制开关。第一个开关，负责控制继续保存长期状态c；第二个开关，负责控制把即时状态输入到长期状态c；第三个开关，负责控制是否把长期状态c作为当前的LSTM的输出。三个开关的作用如下图所示：

![img](http://upload-images.jianshu.io/upload_images/2256672-bff9353b92b9c488.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

接下来，我们要描述一下，输出h和单元状态c的具体计算方法。