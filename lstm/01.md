# 什么是 LSTMs

本章节介绍了 LSTMs 及其工作原理。完成本课后，你将会知道：

- 什么是序列预测，以及它们与一般预测建模问题有何不同。
- 多层感知器在序列预测方面的局限性，递归神经网络在序列预测方面的前景，以及 LSTMs 如何实现这一前景。
- 令人印象深刻的 LSTMs 在挑战序列预测问题上的应用，以及对 LSTMs 的一些局限性的警告。

## 序列预测问题

序列预测不同于其他类型的监督学习问题。**在训练模型和进行预测时，必须保持观察结果的顺序。**通常，涉及序列数据的预测问题被称为序列预测问题，尽管根据输入和输出序列有一系列不同的问题。这一节将研究 4 种不同类型的序列预测问题：

1. Sequence Prediction.
2. Sequence Classification.
3. Sequence Generation.
4. Sequence-to-Sequence Prediction.

但首先，要弄清楚**集合（set）**和**序列（sequence）**之间的区别。

### Sequence

在应用机器学习中，我们经常处理集合，例如一列或一组样本的测试集。集合中的每个样本都可以看作是定义域中的一个观察值。在一个集合中，观察的顺序并不重要。

序列是不同的。序列对观测结果施加了明确的顺序。顺序很重要。在使用序列数据作为模型的输入或输出的预测问题的制定过程中，必须考虑到这一点。

### Sequence Prediction

序列预测包括预测给定输入序列的下一个值。例如：

```shell
Input Sequence: 1, 2, 3, 4, 5
Output Sequence: 6
```

![epiction of a sequence prediction problem.][lstm-1-1]

序列预测一般也称为序列学习。从技术上讲，我们可以把下列所有问题都看作是一种序列预测问题。这可能会使初学者感到困惑。

> _序列数据的学习一直是模式识别和机器学习的基本任务和挑战。涉及顺序数据的应用程序可能需要预测新事件、生成新序列或决策，如序列或子序列的分类。_

一般来说，在这本书中，我们将使用**“序列预测”**来指代一般类型的序列数据预测问题。然而，在本节中，我们将把序列预测与其他形式的预测区别开来，将序列数据定义为下一个时间步的预测。

一些序列预测问题的例子包括：

- **天气预报**。根据一段时间内对天气的一系列观察，预测明天的天气。
- **股市预测**。 给定安全性随时间推移的一系列运动，请预测安全性的下一个运动。
- **产品推荐**。 给定客户过去的购买顺序，请预测客户的下一次购买。

### Sequence Classification

序列分类涉及到预测给定输入序列的类标签。例如:

```shell
Input Sequence: 1, 2, 3, 4, 5
Output Sequence: "good"
```

![Depiction of a sequence classification problem.][lstm-1-2]

**序列分类的目的**：利用标记数据集[…]建立分类模型。因此，该模型可用于预测未知序列的类标签。

**输入序列可以由实值或离散值组成**。在后一种情况下，这类问题可称为离散序列分类问题。一些序列分类问题的例子包括:

- **DNA 序列分类**。给定 a、C、G 和 T 值的 DNA 序列，预测该序列是编码区还是非编码区。
- **异常检测**。 给定一系列观察结果，请预测该序列是否异常。
- **情感分析**。 给定一系列文本（例如评论或推文），预测文本的情绪是正面还是负面。

### Sequence Generation

序列生成包括生成与语料库中的其他序列具有相同一般特征的新输出序列。例如:

```shell
Input Sequence: [1, 3, 5], [7, 9, 11]
Output Sequence: [3, 5 ,7]
```

![Depiction of a sequence generation problem.][lstm-1-3]

> [递归神经网络]可以通过一步一步地处理真实的数据序列并预测接下来会发生什么来训练序列生成。假设预测是概率性的，通过对网络的输出分布进行迭代采样，然后将样本作为下一步的输入，可以从训练好的网络中生成新的序列。换句话说，通过让网络把它的发明当作是真实的，就像在做梦一样。
>
> — Generating Sequences With Recurrent Neural Networks, 2013.

一些序列生成问题的例子包括：

- **文字生成**。 给定一个文本语料库（例如莎士比亚的作品），生成新的句子或文本段落，阅读它们可能是从该语料库中提取的。
- **手写预测**。 给定一个手写示例的语料库，为具有语料库中笔迹特性的新短语生成笔迹。
- **音乐生成**。 给定一个音乐实例集，生成具有该音乐集特性的新音乐作品。

序列的产生也可以指代给定单个观察作为输入的序列的产生。 一个示例是图像的自动文本描述。

- **图像标题生成**。 给定图像作为输入，生成描述图像的单词序列。

例如：

```shell
Input Sequence: [image pixels]
Output Sequence: ["man riding a bike"]
```

![Depiction of a sequence generation problem for captioning an image.][lstm-1-4]

> 能够用适当的英语句子自动描述图像的内容是一项非常具有挑战性的任务，但它可能会产生巨大的影响[…]事实上，一个描述不仅必须捕获图像中包含的对象，而且还必须表达这些对象如何相互关联，以及它们的属性和它们所涉及的活动。
>
> — Show and Tell: A Neural Image Caption Generator, 2015

### Sequence-to-Sequence Prediction

序列到序列的预测涉及到给定输入序列的输出序列的预测。例如:

```shell
Input Sequence: 1, 2, 3, 4, 5
Output Sequence: 6, 7, 8, 9, 10
```

![Depiction of a sequence-to-sequence prediction problem.][lstm-1-5]

> 尽管深度神经网络具有灵活性和强大的功能，但它只适用于那些输入和目标可以用固定维数的向量合理编码的问题。这是一个重要的限制，因为许多重要的问题最好用长度未知的序列来表示。例如，语音识别和机器翻译是连续的问题。同样，回答问题也可以看作是将表示问题的单词序列映射到表示答案的单词序列。
>
> — Sequence to Sequence Learning with Neural Networks, 2014.

序列到序列预测是序列预测的一种微妙但具有挑战性的扩展，它不是预测序列中的一个单独的下一个值，而是预测一个可能与输入序列具有相同长度或相同时间的新序列。这种类型的问题最近在自动文本翻译(例如，将英语翻译成法语)领域中得到了大量的研究，可以用缩写 seq2seq 来指代。

> seq2seq 学习的核心是使用递归神经网络将可变长度的输入序列映射到可变长度的输出序列。虽然 seq2seq 方法相对较新，但在……机器翻译方面已经取得了最先进的成果。
>
> — Multi-task Sequence to Sequence Learning, 2016.

如果输入和输出序列是时间序列，则该问题可以称为**多步时间序列预测（multi-step time series forecasting）**。 序列间问题的一些示例包括：

- **多步时间序列预测**。 给定一个时间序列的观测值，可以预测一系列未来时间步长的观测值序列。
- **文字摘要**。 给定一个文本文件，该文件描述了源文件的重要部分。
- **程序执行**。 给定文本描述程序或数学方程式，可以预测描述正确输出的字符序列。

## Limitations of Multilayer Perceptrons

经典的神经网络称为**多层感知器**，简称 MLPs，可以应用于序列预测问题。mlp 近似一个从输入变量到输出变量的映射函数。这种通用能力对于序列预测问题(尤其是时间序列预测)是有价值的，原因有很多。

- **对噪声的鲁棒性（Robust to Noise）**。 神经网络对于输入数据和映射功能中的噪声具有鲁棒性，甚至在缺少值的情况下甚至可以支持学习和预测。
- **非线性**。神经网络对映射函数没有很强的假设，容易学习线性和非线性关系。

更具体地说，可以将 mlp 配置为支持映射函数中任意定义但固定数量的输入和输出。这意味着:

- **多变量输入**。可以指定任意数量的输入特征，为多元预测提供直接支持。
- **多步骤的输出**。可以指定任意数量的输出值，为多步甚至多变量预测提供直接支持。

这种能力克服了使用传统线性方法(如用于时间序列预测的 ARIMA 等工具)的限制。单就这些能力而言，前馈神经网络在时间序列预测中得到了广泛的应用。

MLPs 在序列预测中的应用要求将输入序列分割成较小的重叠子序列，并将这些重叠子序列显示给网络以生成预测。输入序列的时间步长成为网络的输入特征。子序列是重叠的，以模拟一个窗口沿序列滑动，以生成所需的输出。这可以很好地解决一些问题，但它有 5 个关键的局限性。

- **无状态的（Stateless）**。mlp 学习一个固定的函数近似值。任何依赖于输入序列上下文的输出都必须被一般化（generalized）并固定（frozen）到网络权值中。
- **不用关心时间结构（Unaware of Temporal Structure）**。 时间步长被建模为输入特征，这意味着网络对观察之间的时间结构或顺序没有明确的处理或理解。
- **混乱缩放（Messy Scaling）**：对于需要对多个并行输入序列进行建模的问题，输入特征的数量会随着滑动窗口大小的增加而增加，而无需明确区分序列的时间步长。
- **固定尺寸的输入（Fixed Sized Inputs）**：滑动窗口的大小是固定的，必须施加到网络的所有输入上。
- **固定大小的输出（Fixed Sized Outputs）**。 输出的大小也是固定的，任何不符合要求的输出都必须强制执行

**MLP**确实为序列预测提供了强大的功能，但仍然受到这一关键限制的影响，即**必须在模型的设计中明确指定观测值之间的时间相关性范围。** mlp 是建模序列预测问题的一个很好的起点，但是我们现在有了更好的选择。

### Promise of Recurrent Neural Networks

长短时记忆(LSTM)网络是一种递归神经网络。递归神经网络(RNNs)是一种特殊的序列问题神经网络。给出一个标准的前馈 MLP 网络，一个 RNN 可以被认为是该体系结构中增加的循环。例如，在一个给定的层中，每个神经元除了向下一层传递信号外，还可以向后面(侧面)传递信号。网络的输出可以反馈为网络的输入，并带有下一个输入向量。等等。

重复连接将 state 或 memory 添加到网络中，使其能够学习和利用输入序列中观察到的有序特性。

> …递归神经网络包含一些周期，这些周期将先前时间步长的网络活动作为输入输入到网络中，从而影响当前时间步的预测。这些激活被存储在网络的内部状态中，这些状态在原则上可以保存长期的时间上下文信息。这种机制允许 RNNs 在输入序列历史记录上利用动态变化的上下文窗口
>
> — Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling, 2014.

> _长短时记忆(LSTM)能够利用固定大小的时间窗来解决前馈网络无法解决的许多时间序列任务。_
>
> — Applying LSTM to Time Series Predictable through Time-Window Approaches, 2001.

**除了使用神经网络进行序列预测的一般好处之外，RNNs 还可以学习和利用数据的时间依赖性。**也就是说，在最简单的情况下，网络可以从一个序列中一次显示一个观察结果，并可以了解它之前看到的哪些观察结果是相关的，以及它们与做出预测的关系如何。

> 由于 LSTM 网络能够学习序列中的长期相关性，因此不需要预先指定的时间窗口，并且能够准确地建模复杂的多元序列。
>
> — Long Short Term Memory Networks for Anomaly Detection in Time Series, 2015.\*

递归神经网络的前景是可以学习输入数据中的时间依赖性和上下文信息。

> 输入不是固定而是组成输入序列的递归网络可用于将输入序列转换为输出序列，同时以灵活的方式考虑上下文信息。
>
> — Learning Long-Term Dependencies with Gradient Descent is Diffcult, 1994.

有许多 RNNs，但是 LSTM 实现了 RNNs 对序列预测的承诺。这就是为什么现在有这么多关于 LSTMs 的讨论和应用。

**LSTMs 具有内部状态，它们明确地知道输入中的时间结构，能够分别对多个并行输入序列进行建模，并且能够单步通过不同长度的输入序列生成可变长度的输出序列，每次一个观察结果。**

接下来，让我们进一步了解 LSTM 网络。

## The Long Short-Term Memory Network

LSTM 网络不同于传统的 MLP。 像 MLP 一样，网络由神经元层组成。 输入数据通过网络传播，以便进行预测。

与 RNNs 一样，LSTMs 也有重复的连接，因此来自前一个时间步的神经元激活的状态被用作生成输出的上下文。但与其他 RNN 不同的是，LSTM 有一个独特的配方，它可以避免其他 RNN 的训练和规模的问题。这一点，以及可以实现的令人印象深刻的结果，是该技术流行的原因。

RNN 一直以来面临的关键技术挑战是如何有效地训练它们。实验表明，权重更新程序导致权重变化迅速变小，导致梯度消失，或导致非常大的变化甚至溢出（爆炸梯度）。 LSTM 克服了这一挑战。

> 不幸的是，标准的 RNNs 所能访问的上下文信息的范围实际上是相当有限的。问题是，给定的输入对隐含层的影响，以及因此对网络输出的影响，在围绕网络的循环连接时，要么衰减，要么呈指数级放大。这个缺点…在文献中被称为消失梯度问题…长短时记忆(LSTM)是一种专门针对消失梯度问题设计的 RNN 结构。
>
> — A Novel Connectionist System for Unconstrained Handwriting Recognition, 2009.

> LSTM 架构是通过对现有 RNN 中的错误流进行分析而得出的，该分析发现，现有架构无法访问长时间的信息，因为反向传播的错误会导致梯度爆炸或呈指数级衰减。LSTM 层由一组递归连接的块组成，称为内存块。这些块可以被认为是数字计算机中存储芯片的可微版本。每个单元包含一个或多个递归连接的存储单元和三个乘法单元——输入、输出和遗忘门，它们为单元提供连续的写、读和重置操作。网络只能通过门与细胞相互作用。
>
> — Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures, 2005.

### LSTM Weights

记忆单元（memory cell）具有用于输入，输出以及通过暴露于输入时间步长而建立的内部状态的权重参数。

- **输入权重**。用于对当前时间步长的输入进行加权。
- **输出权值**。用于对最后一个时间步的输出进行加权。
- **内部状态（Internal State）**。内部状态用于计算此时间步长的输出。

### LSTM Gates

记忆单元的关键是门（gate）。这些都是加权函数，进一步控制单元中的信息流。有三个门:

- Forget Gate（遗忘门）：决定从单元中丢弃哪些的信息。
- Input Gate（输入门）：决定哪些输出的值去更新记忆状态。
- Outut Gate（输出门）：根据输入和记忆单元确定输出什么。

**遗忘门和输入门用于内部状态的更新。输出门是单元实际输出内容的最终限制器。这些门和被称为 constant error carrousel（CEC）的一致的数据流使每个单元保持稳定(既不爆炸也不消失)。**

> 每个存储单元的内部结构保证了恒定的误差，在恒定的误差范围内。这是桥接很长时间滞后的基础。两个门单元学习在每个记忆单元的 CEC 中打开和关闭对错误流的访问。乘法输入门（multiplicative input gate）提供了对 CEC 的保护，使其不受无关输入的干扰。同样，乘法输出门保护其他单元不受当前不相关的记忆内容的干扰。
>
> — Long Short-Term Memory, 1997.

与传统的 MLP 神经元不同，LSTM 内存单元很难清晰地绘制出来。到处都是线、砝码和门。如果您认为 LSTM 内部的图片或基于等式的描述将进一步有所帮助，请参阅本章末尾的一些参考资料。我们可以总结出 LSTMs 的三个主要优点：

- **克服了训练 RNN 的技术问题，即梯度消失和爆炸。**
- **具有记忆，以克服长期时间依赖与输入序列的问题。**
- **按时间步长处理输入序列和输出序列，允许可变长度的输入和输出。**

## Applications of LSTMs

LSTMs 可以为具有挑战性的序列预测问题提供优雅的解决方案。本节将提供 3 个示例，以便提供 LSTMs 能够实现的结果的快照。

### Automatic Image Caption Generation

自动图像字幕是这样一种任务:给定一幅图像，系统必须生成描述图像内容的字幕。2014 年，大量的深度学习算法在这个问题上取得了令人印象深刻的结果，这些算法利用顶级模型的工作来进行对象分类和照片中的对象检测。

一旦您可以在照片中检测对象并为这些对象生成标签，您就可以看到下一步是将这些标签转换为连贯的句子描述。该系统使用非常大的卷积神经网络来检测照片中的目标，然后使用 LSTM 将标签转换成连贯的句子。

![xample of LSTM generated captions, taken from Show and Tell: A Neural Image
Caption Generator, 2014. ][lstm-1-6]

### Automatic Translation of Text

自动文本翻译是这样一种任务：给定一种语言的文本句子，把它们翻译成另一种语言的文本。例如，英语句子作为输入，中文句子作为输出。该模型必须学习单词的翻译、修改翻译的上下文，并支持输入和输出序列，这些序列的长度可能在一般情况下和相互之间有所不同。

![Example of English text translated to French comparing predicted to expected
translations, taken from Sequence to Sequence Learning with Neural Networks, 2014.][lstm-1-7]

### Automatic Handwriting Generation

这是一个任务，在给定的手写示例库中，为给定的单词或短语生成新的手写。在创建手写样本时，手写作为笔使用的坐标序列提供。从这个语料库中，可以学习笔的运动和字母之间的关系，并生成新的例子。有趣的是，不同的风格可以学习，然后模仿。我很乐意看到这项工作结合一些法医笔迹分析专家。

![ Example of LSTM generated captions, taken from Generating Sequences With
Recurrent Neural Networks, 2014. ][lstm-1-8]

## Limitations of LSTMs

LSTMs 非常令人印象深刻。该网络的设计克服了神经网络的技术挑战，实现了神经网络序列预测的承诺。LSTMs 在一系列复杂序列预测问题中的应用取得了良好的效果。但是 LSTMs 并不适用于所有的序列预测问题。

例如，在时间序列预测中，与预测相关的信息通常在过去观测的一个小窗口内。通常，带有窗口或线性模型的 MLP 可能是较简单且更适合的模型。

> 在文献中发现的时间序列基准问题…通常比 LSTM 已经解决的许多任务在概念上更简单。它们通常根本不需要 RNN，因为关于下一个事件的所有相关信息都是通过包含在一个小时间窗口内的几个最近事件来传递的。
>
> — Applying LSTM to Time Series Predictable through Time-Window Approaches, 2001

LSTMs 的一个重要限制是记忆（memory）。或者更准确地说，记忆是如何被滥用的。有可能迫使 LSTM 模型在很长的输入时间步长的情况下记住单个观察结果。这是对 LSTM 的糟糕使用，并且需要 LSTM 模型来记住多个观察值将会失败。

这可以在将 LSTMs 应用于时间序列预测时看到，在这种情况下，问题被表述为一个自回归，该自回归要求输出是输入序列中多个距离时间步长的函数。LSTM 可能会被迫执行这个问题，但是它的效率通常比精心设计的自回归模型或问题的重新构造要低。

> 假设任何动态模型都需要 t-tau 的所有输入…，我们注意到[自回归]-RNN 必须存储从 t-tau 到 t 的所有输入，并在适当的时间覆盖它们。这需要实现一个循环缓冲区，这是 RNN 很难模拟的结构。
>
> — Applying LSTM to Time Series Predictable through Time-Window Approaches, 2001.

需要注意的是，使用 lstm 需要仔细考虑你问题得框架。可以将 LSTMs 的内部状态看作是一个方便的内部变量，以便捕获和提供用于进行预测的上下文。如果问题看起来像一个传统的自回归类型的问题，在一个小窗口中有最相关的滞后观察，那么在考虑 LSTM 之前，可以使用 MLP 和滑动窗口开发一个性能基线。

## Further Reading

如果您想更深入地了解算法的技术细节，请阅读以下有关 LSTM 的必读文章。

### Sequence Prediction Problems

- Sequence on Wikipedia.
  <https://en.wikipedia.org/wiki/Sequence>
- On Prediction Using Variable Order Markov Models, 2004.
- Sequence Learning: From Recognition and Prediction to Sequential Decision Making, 2001.
- Chapter 14, Discrete Sequence Classification, Data Classification: Algorithms and Appli-cations, 2015.
  <http://amzn.to/2tkM723>
- Generating Sequences With Recurrent Neural Networks, 2013.
  <https://arxiv.org/abs/1308.0850>
- Show and Tell: A Neural Image Caption Generator, 2015.
  <https://arxiv.org/abs/1411.4555s>
- Multi-task Sequence to Sequence Learning, 2016.
  <https://arxiv.org/abs/1511.06114>
- Sequence to Sequence Learning with Neural Networks, 2014.
  <https://arxiv.org/abs/1409.3215>
- Recursive and direct multi-step forecasting: the best of both worlds, 2012.

### MLPs for Sequence Prediction

- Neural Networks for Time Series Processing, 1996.
- Sequence to Sequence Learning with Neural Networks, 2014.
  https://arxiv.org/abs/1409.3215

### Promise of RNNs

- Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling, 2014 .
- Applying LSTM to Time Series Predictable through Time-Window Approaches, 2001.
- Long Short Term Memory Networks for Anomaly Detection in Time Series, 2015.
- Learning Long-Term Dependencies with Gradient Descent is Difficult, 1994.
- On the difficulty of training Recurrent Neural Networks, 2013.
  <https://arxiv.org/abs/1211.5063>

### LSTMs

- Long Short-Term Memory, 1997.
- Learning to forget: Continual prediction with LSTM, 2000.
- A Novel Connectionist System for Unconstrained Handwriting Recognition, 2009.
- Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures, 2005

### LSTM Applications

- Show and Tell: A Neural Image Caption Generator, 2014 .
  <https://arxiv.org/abs/1411.4555>
- Sequence to Sequence Learning with Neural Networks, 2014.
  <https://arxiv.org/abs/1409.3215>
- Generating Sequences With Recurrent Neural Networks, 2014.
  <https://arxiv.org/abs/1308.0850>

## Extensions

[lstm-1-1]: ../.gitbook/assets/lstm/1-1.png
[lstm-1-2]: ../.gitbook/assets/lstm/1-2.png
[lstm-1-3]: ../.gitbook/assets/lstm/1-3.png
[lstm-1-4]: ../.gitbook/assets/lstm/1-4.png
[lstm-1-5]: ../.gitbook/assets/lstm/1-5.png
[lstm-1-6]: ../.gitbook/assets/lstm/1-6.png
[lstm-1-7]: ../.gitbook/assets/lstm/1-7.png
[lstm-1-8]: ../.gitbook/assets/lstm/1-8.png
