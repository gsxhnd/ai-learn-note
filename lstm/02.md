## How to Train LSTMs

本章节讲解了用于训练 LSTMs 的时间反向传播算法。完成本课后，你将会知道：

- 什么是时间的反向传播，以及它如何与多层感知器网络使用的反向传播训练算法相关联。
- 导致需要通过时间进行**截断反向传播（Truncated Backpropagation）**的动机，这是 LSTMs 深度学习中最广泛使用的变体。
- 考虑配置“经过时间截断的反向传播”以及研究和深度学习库中使用的规范配置的概念。
- 关于配置“经过时间截断的反向传播”以及研究和深度学习库中使用的规范配置的一种表示法。

### Backpropagation Training Algorithm

反向传播指的是两件事：

- 导数计算的数学方法及导数链式法则的应用。
- 更新网络权值以最小化误差的训练算法。

本课中使用的正是后一种对反向传播的理解。反向传播训练算法的目标是修改神经网络的权值，使网络输出相对于相应输入的期望输出的误差最小化。它是一种监督学习算法，允许网络根据特定的错误进行纠正。一般算法如下:

1. 提供一个训练输入模式，并通过网络传播它以获得输出。
2. 将预测输出与预期输出进行比较，并计算误差。
3. 计算误差对网络权值的导数。
4. 调整权重以使误差最小化。
5. 重复。

### Unrolling Recurrent Neural Networks

递归神经网络的一个简单概念是作为一种神经网络，它从先前的时间步中获取输入。 我们可以用一个图表来证明这一点。

![lstm-02-01][lstm-02-01]

RNNs是适合的，并在许多时间步长的预测。随着时间步长的增加，具有循环连接的简单图开始失去所有意义。我们可以通过在输入序列上展开或展开RNN图来简化模型。



#### Unfolding the Forward Pass

考虑这样一种情况，我们有多个输入`(X(t)， X(t+1)，…)`的时间步长，多个内部状态`(u(t)， u(t+1)，…)`的时间步长，以及多个输出`(y(t)， y(t+1)，…)`的时间步长。我们可以将网络示意图展开成一个没有任何循环的图，如下图所示。

![lstm-02-02][lstm-02-02]

可以看到，我们移动的循环和上一个时间步骤的输出(y(t))和内部状态(u(t))作为处理下一个时间步骤的输入被传递到网络。这个概念的关键是网络(RNN)在展开的时间步之间不发生变化。具体来说，每个时间步使用相同的权值，只有输出和内部状态不同。通过这种方式，整个网络(拓扑结构和权值)在输入序列中的每个时间步都被复制。

我们可以将这一概念进一步推广，将网络的每个副本视为同一前馈神经网络的一个附加层。**较深的层作为输入前一层的输出以及一个新的输入时间步长**。这些层实际上是同一组权值的所有副本，内部状态从一层更新到另一层，这可能是这个经常使用的类比的延伸。

![lstm-02-03][lstm-02-03]

> RNNs沿着时间维度展开，就可以被看作是所有层具有相同权值的深度前馈网络。
>
> — Deep learning, Nature, 2015.

这是一个有用的概念工具和可视化工具，有助于理解在转发过程中网络中发生的事情。这可能是也可能不是深度学习库实现网络的方式。



#### Unfolding the Backward Pass

网络展开的思想在递归神经网络实现后向遍历的过程中起着越来越重要的作用。

> _与[时间反向传播]的标准一样，网络是随时间展开的，因此到达各层的连接被视为来自前一个时间步长。
> — Framewise phoneme classification with bidirectional LSTM and other neural network architectures, 2005._

**重要的是，给定时间步的误差反向传播取决于网络在前一时间步的激活。这样，反向传递需要展开网络的概念化。误差被传播回序列的第一个输入时间步，以便计算误差梯度并更新网络的权值。**

> _和标准的反向传播一样，[时间反向传播]由链规则的重复应用组成。其微妙之处在于，对于递归网络，损失函数不仅依赖于隐藏层对输出层的影响，而且还依赖于它在下一个时间步对隐藏层的影响。_
>
> _— Supervised Sequence Labelling with Recurrent Neural Networks, 2008._

展开递归网络图也会引入其他关注点。每一个时间步都需要一个新的网络副本，而这又需要更多的记忆，特别是对于具有数千或数百万权重的大型网络。训练大型递归网络的记忆需求会随着时间步数上升到数百步而迅速膨胀。

> _… 需要按输入序列的长度展开RNN。通过展开一个RNN N次，网络中神经元的每一次激活都被复制N次，这会消耗大量的内存，特别是当序列很长的时候。这阻碍了在线学习或适应的小规模实施。此外，这种“完全展开”使得在共享内存模型（如图形处理单元（GPU））上使用多个序列进行并行训练成为可能。_
>
> _— Online Sequence Training of Recurrent Neural Networks with Connectionist Temporal Classification, 2015._

### Backpropagation Through Time

时间反向传播（BPTT）是反向传播训练算法在递归神经网络中的应用。在最简单的情况下，递归神经网络每一个时间步显示一个输入，并预测一个输出。

从概念上讲，BPTT通过展开所有输入时间步骤来工作。每个时间步都有一个输入时间步、一个网络副本和一个输出。然后计算并累积每个时间步的误差。网络将回滚并更新权重。算法总结如下：

1. 向网络显示输入和输出对的时间步序列。
2. 展开网络，然后计算并累积每个时间步骤中的错误。
3. 卷起（Roll-up）网络并更新权重。
4. 重复。

随着时间步数的增加，BPTT的计算开销可能会增加。如果输入序列由数千个时间步组成，则这将是单个权重更新所需的导数数。这可能会导致权重消失或爆炸（归零或溢出），并使缓慢的学习和模型技能变得嘈杂。

> _BPTT的一个主要问题是单参数更新的代价很高，这使得无法使用大量的迭代。_
>
> _— Training Recurrent Neural Networks, 2013._

一种最小化爆炸和消失梯度问题的方法是在对权重进行更新之前限制多少时间步。

### Truncated Backpropagation Through Time

**截断时间反向传播（TBPTT）**是递归神经网络BPTT训练算法的改进版本，该算法一次处理一个时间步长，并定期对固定数量的时间步长进行更新。

> _截断的BPTT。。。一次处理一个时间步，每 k1 个时间步，它运行 k2 时间步的BPTT，所以如果k2很小，参数更新就很便宜。因此，它的隐藏状态暴露在许多时间步骤中，因此可能包含有关遥远过去的有用信息，这些信息将被机会主义地利用。_
>
> _— Training Recurrent Neural Networks, 2013._

算法总结如下：

1. 向网络呈现输入和输出对的k1时间步序列。
2. 展开网络，然后计算并累积k2时间步的错误。
3. 卷起网络并更新权重。
4. 重复。

TBPTT算法需要考虑两个参数：

- **k1**：更新之间的前向传递时间步数。通常，考虑到权重更新的频率，这会影响训练的速度或速度。
- **k2**：应用BPTT的时间步数。一般来说，它应该足够大，能够捕获问题中的时间结构，以便网络学习，值太大会梯度导致渐变消失。

### Configurations for Truncated BPTT

我们可以更进一步，定义一个符号来帮助更好地理解BPTT。Williams和Peng在他们对BPTT的研究中提出了一种*有效的基于梯度的递归网络轨迹在线训练算法*，他们设计了一种符号来捕获截断和非截断配置的频谱，例如BPTT（h）和BPTT（h；1）。

我们可以修改这个符号并使用Sutskever的k1和k2参数。使用这个符号，我们可以定义一些标准或通用的方法：注意：这里n是指输入序列中的时间步总数：

- TBPTT（n，n）：在序列结束时，在序列中的所有时间步执行更新（例如，经典BPTT）。
- TBPTT（1，n）：时间步一次处理一个，然后更新，覆盖到目前为止看到的所有时间步（例如威廉斯和彭的经典TBPTT）。
- TBPTT（k1,1）：网络可能没有足够的时间上下文来学习，严重依赖于内部状态和输入。
- TBPTT(k1,k2), where k1<k2<n：每个序列执行多个更新，可以加速训练。
- TBPTT（k1，k2），其中k1=k2：一种通用配置，其中固定数量的时间步用于向前和向后通过时间步（例如10s到100s）。

我们可以看到，所有的配置都是TBPTT（n，n）上的一个变体，本质上试图以更快的训练和更稳定的结果来近似相同的效果。文献中提出的典型TBPTT可被认为是TBPTT（k1，k2），w h e r e k1=k2=k和k<=n，其中所选参数很小（几十到几百个时间步）。这里，k是一个必须指定的参数。通常认为输入时间步的序列长度应限制在200-400之间。

### Keras Implementation of TBPTT

Keras深度学习lib库提供了一种TBPTT的训练神经网络的实现方法。实现比上面列出的一般版本受到更多限制。具体来说，k1和k2值彼此相等且固定。

```shell
TBPTT(k1, k2), where k1=k2=k.
```

这是通过训练像LSTM这样的递归神经网络所需的固定大小的三维输入来实现的。LSTM期望输入数据具有以下维度：**样本、时间步长和特征**。它是这个输入格式的第二个维度，时间步，定义了在序列预测问题上用于向前和向后传递的时间步数。

因此，在为Keras中的序列预测问题准备输入数据时，必须仔细选择指定的时间步数。时间步骤的选择将影响以下两个方面：

- **前进过程中积累的内部状态**。
- **用于更新后向通道上权重的梯度估计**。

请注意，默认情况下，网络的内部状态在每个批处理之后重置，但是可以通过使用所谓的有状态LSTM并手动调用重置操作来更明确地控制何时重置内部状态。稍后再谈。

该算法的Keras实现本质上是不截断的，要求在训练模型之前直接对输入序列执行任何截断。我们可以认为这是手动截断的BPTT。Sutskever称这是一个幼稚的方法。

> _… 一种简单的方法，将1000个长序列分成50个序列（比如说，每个序列的长度为20），并将每个序列的长度为20作为一个单独的训练案例。这是一种明智的方法，可以在实践中很好地工作，但它对跨越20多个时间步的时间依赖性是盲目的。_
>
> _— Training Recurrent Neural Networks, 2013_

这意味着作为构建问题框架的一部分，您必须将长序列分割成子序列，子序列的长度既足以捕获进行预测的相关上下文，又足以对网络进行有效的训练。

### Further Reading

#### Books

- Neural Smithing, 1999.
  <http://amzn.to/2u9yjJh>
- Deep Learning, 2016 .
  <http://amzn.to/2sx7oFo>
- Supervised Sequence Labelling with Recurrent Neural Networks, 2008.
  <http://amzn.to/2upsSJ9>

#### Research Papers

- Online Sequence Training of Recurrent Neural Networks with Connectionist Temporal Classification, 2015.

  <https://arxiv.org/abs/1511.06841>

- Framewise phoneme classification with bidirectional LSTM and other neural network architectures, 2005.

- Deep learning, Nature, 2015.

- Training Recurrent Neural Networks, 2013.

- Learning Representations By Backpropagating Errors, 1986.

- Backpropagation Through Time: What It Does And How To Do It, 1990.

- An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories, 1990.

- Gradient-Based Learning Algorithms for Recurrent Networks and Their Computational Complexity, 1995.


[lstm-02-01]: ../.gitbook/assets/lstm/02-01.png
[lstm-02-02]: ../.gitbook/assets/lstm/02-02.png
[lstm-02-03]: ../.gitbook/assets/lstm/02-03.png

