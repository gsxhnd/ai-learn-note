# How to Develop LSTMs in Keras

## Define the Model

第一步是定义网络。在 Keras 中，神经网络被定义为一系列的层。这些层的容器是`Sequential`类。第一步是创建`Sequential`类的实例。然后，您可以创建图层并按它们应该连接的顺序添加它们。由内存单元组成的 LSTM 递归层称为`LSTM()`。通常紧跟在 LSTMlayers 之后并用于输出预测的完全连接层称为`Dense()`。

例如，我们可以定义一个包含 2 个存储单元的`LSTM`隐藏层，然后定义一个包含 1 个神经元的密集输出层，如下所示：

```python
model = Sequential()
model.add(LSTM(2))
model.add(Dense(1))
```

但我们也可以通过创建一个层数组并将其传递给序列类的构造器，在一个步骤中完成这项工作。

```python
layers = [LSTM(2), Dense(1)]
model = Sequential(layers)
```

网络中的第一个隐藏层必须定义预期的输入数量，例如输入层的形状。输入必须是三维的，按顺序由样本、时间步和特征组成。

1. Samples. These are the rows in your data. One sample may be one sequence.
2. Time steps. These are the past observations for a feature, such as lag variables.
3. Features. These are columns in your data.

假设您的数据是作为 NumPy 数组加载的，那么您可以使用 NumPy 中的 shape（）函数将 1D 或 2D 数据集转换为 3D 数据集。您可以在 NumPy 数组上调用 reshape（）函数，并向它传递一个要转换数据的维度元组。假设我们在一个 NumPy 数组中有两列输入数据（X）。我们可以将这两个列视为两个时间步骤，并对其进行如下整形：

```python
data = data.reshape((data.shape[0], data.shape[1], 1))
```

如果要使二维数据中的列成为一个时间步长的要素，可以按以下方式对其进行重塑：

```python
data = data.reshape((data.shape[0], 1, data.shape[1]))
```

可以指定输入形状参数，该参数需要一个包含时间步数和特征数的元组。例如，如果一个单变量序列有两个时间步和一个特征，每行有两个滞后观测值，则可以指定如下：

```python
model = Sequential()
model.add(LSTM(5, input_shape=(2,1)))
model.add(Dense(1))
```

不必指定样本数。该模型假设有一个或多个示例，您只需定义时间步数和功能。本课的最后一节提供了为 LSTM 模型准备输入数据的其他示例。

把一个序列模型想象成一条管道，一端输入原始数据，另一端输出预测。这是 Keras 中一个有用的容器，因为传统上与层相关联的关注点也可以作为单独的层拆分和添加，清楚地显示了它们在数据从输入到预测的转换中的作用。例如，可以提取转换来自层中每个神经元的总和信号的激活函数，并将其作为一个类似层的对象添加到序列中，称为激活。

```python
model = Sequential()
model.add(LSTM(5, input_shape=(2,1)))
model.add(Dense(1))
model.add(Activation(sigmoid))
```

激活函数的选择对于输出层是最重要的，因为它将定义预测将采用的格式。例如，下面是一些常见的预测建模问题类型以及可以在输出层中使用的结构和标准激活函数：

1. Regression: Linear activation function, or linear, and the number of neurons matching the number of outputs. This is the default activation function used for neurons in the Dense layer.
2. Binary Classification (2 class): Logistic activation function, or sigmoid, and one neuron the output layer.
3. Multiclass Classification (> 2 class): Softmax activation function, or softmax, and one output neuron per class value, assuming a one hot encoded output pattern.

## Compile the Model

一旦我们定义了我们的网络，我们就必须编译它。编译是一个高效的步骤。它将我们定义的简单层序列转换为高效的矩阵转换序列，其格式将在 GPU 或 CPU 上执行，具体取决于 Keras 的配置方式。将编译视为网络的预计算步骤。定义模型后总是需要它。

编译需要指定许多参数，特别是为训练网络而定制的参数。具体来说，用优化算法来训练网络，用损失函数来评估被优化算法最小化的网络。

例如，下面是为回归型问题编译定义的模型并指定随机梯度下降（sgd）优化算法和均方误差（mse）损失函数的情况。

```python
model.compile(optimizer= sgd , loss= mse)
```

或者，优化器可以在作为编译步骤的参数提供之前创建和配置。

```python
algorithm = SGD(lr=0.1, momentum=0.3)
model.compile(optimizer=algorithm, loss= mse )
```

预测建模问题的类型对可使用的损失函数的类型施加约束。例如，以下是不同预测模型类型的一些标准损耗函数：

1. Regression: Mean Squared Error or mean squared error, mse for short.
2. Binary Classification (2 class): Logarithmic Loss, also called cross entropy or binary crossentropy.
3. Multiclass Classification (> 2 class): Multiclass Logarithmic Loss or
   categorical crossentropy.

最常见的优化算法是经典的随机梯度下降，但 Keras 也支持这一经典优化算法的一系列其他扩展，这些扩展在很少或没有配置的情况下都能很好地工作。也许最常用的优化算法，因为它们通常有更好的性能：

1. Stochastic Gradient Descent, or sgd.
2. Adam, or adam.
3. RMSprop, or rmsprop.

最后，除了 loss 函数之外，还可以指定在拟合模型时要收集的度量。通常，要收集的最有用的附加度量是分类问题的准确性（例如，“准确性”或简称“acc”）。要收集的度量是在度量或损失函数名数组中按名称指定的。例如：

```python
model.compile(optimizer= sgd , loss= mean_squared_error , metrics=[ accuracy ])
```

## Fit the Model

一旦网络被编译，它就可以被拟合，这意味着调整训练数据集上的权重。拟合网络需要指定训练数据，包括输入模式矩阵 X 和匹配输出模式数组 y。使用时间反向传播算法对网络进行训练，并根据编译模型时指定的优化算法和损失函数进行优化。

反向传播算法要求对网络进行训练，以适应训练数据集中指定数量的时间段或暴露于所有序列的情况。每个 epoch 可以被划分成一组称为批的输入输出模式对。这定义了在一个历元内更新权重之前网络暴露的模式数。这也是一种效率优化，确保一次不会有太多的输入模式加载到内存中。

- **Epoch**: One pass through all samples in the training dataset and updating the network weights. LSTMs may be trained for tens, hundreds, or thousands of epochs.

- **Batch**: A pass through a subset of samples in the training dataset after which the network weights are updated. One epoch is comprised of one or more batches.

Below are some common configurations for the batch size:

- batch size=1: Weights are updated after each sample and the procedure is called stochastic gradient descent.
- batch size=32: Weights areupdatedafter a specifiednumber of samples andtheprocedure is called mini-batch gradient descent. Common values are 32, 64, and 128, tailored to the desired efficiency and rate of model updates. If the batch size is not a factor of the number of samples in one epoch, then an additional batch size of the left over samples is run at the end of the epoch.
- batch size=n: Where n is the number of samples in the training dataset. Weights are updated at the end of each epoch and the procedure is called batch gradient descent.

Mini-batch gradient descent with a batch size of 32 is a common configuration for LSTMs. An example of fitting a network is as follows:

```python
model.fit(X, y, batch_size=32, epochs=100)
```

一旦匹配，将返回一个 history 对象，该对象提供模型在训练期间的性能摘要。这包括损失和编译模型时指定的任何其他度量，记录每个 epoch。可以记录、绘制和分析这些度量，以深入了解网络是过度拟合还是欠拟合训练数据。

根据网络规模和培训数据的大小，培训可能需要很长的时间，从几秒钟到几小时，再到几天。默认情况下，命令行上会显示每个历元的进度条。这可能会给您造成太多的噪音，或者可能会给您的环境带来问题，例如您在交互式笔记本或 IDE 中。通过将 verbose 参数设置为 2，可以将显示的信息量减少到每个纪元的损失。通过将 verbose 设置为 0，可以关闭所有输出。例如：

```python
history = model.fit(X, y, batch_size=10, epochs=100, verbose=0)
```

## Evaluate the Model

一旦网络经过训练，就可以对其进行评估。可以根据训练数据对网络进行评估，但这并不能作为预测模型提供网络性能的有用指示，因为它以前已经看到了所有这些数据。我们可以在单独的数据集上评估网络的性能，在测试期间是看不见的。这将提供对网络在预测未来未知数据方面的性能的估计。

模型评估所有测试模式的损失，以及编译模型时指定的任何其他度量，如分类精度。返回一个评估指标列表。例如，对于使用精度度量编译的模型，我们可以在新的数据集上对其进行评估，如下所示：

```python
loss, accuracy = model.evaluate(X, y)
```

与拟合网络一样，提供详细的输出，以了解评估模型的进度。我们可以通过将 verbose 参数设置为 0 来将其设置为 o。

```python
loss, accuracy = model.evaluate(X, y, verbose=0)
```

## Make Predictions on the Model

一旦我们对拟合模型的性能感到满意，我们就可以使用它对新数据进行预测。这与使用新输入模式数组调用模型上的 predict（）函数一样简单。例如：

```python
predictions = model.predict(X)
```

预测将以网络输出层提供的格式返回。在回归问题的情况下，这些预测可以由线性激活函数直接以问题的形式提供。

对于二进制分类问题，预测可以是第一类的概率数组，通过舍入可以转换为 1 或 0。对于多类分类问题，结果可以是概率数组的形式（假设一个热编码输出变量），可能需要使用 argmax（）NumPy 函数将其转换为单个类输出预测。或者，对于分类问题，我们可以使用 predict classes（）函数，该函数将自动将 uncrisp 预测转换为清晰的整数类值。

```python
predictions = model.predict_classes(X)
```

与拟合和评估网络一样，提供详细的输出，以便了解模型进行预测的进度。我们可以通过将 verbose 参数设置为 0 来关闭它。

```python
predictions = model.predict(X, verbose=0)
```

## LSTM State Management

每个 LSTM 存储单元都保持累积的内部状态。这种内部状态可能需要在网络训练期间和进行预测时仔细管理序列预测问题。默认情况下，网络中所有 LSTM 内存单元的内部状态在每次批处理后（例如，更新网络权重时）重置。这意味着批量大小的配置在以下三个方面之间施加了张力：

1. The efficiency of learning, or how many samples are processed before an update.
2. The speed of learning, or how often weights are updated.
3. The influence of internal state, or how often internal state is reset.

Keras 通过将 LSTM 层定义为有状态层，提供了将内部状态的重置与网络权重的更新分离的灵活性。这可以通过将 LSTM 层上的 stateful 参数设置为 True 来完成。使用状态 LSTM 层时，还必须通过设置批输入形状参数将批大小定义为网络定义中输入形状的一部分，并且批大小必须是训练数据集中样本数的一个因子。batch input shape 参数需要定义为批大小、时间步长和功能的三维元组。

例如，我们可以定义一个有状态的 LSTM，它将在一个训练数据集上进行训练，该数据集有 100 个样本，批处理大小为 10，对于一个特征有 5 个时间步，如下所示。

```python
model.add(LSTM(2, stateful=True, batch_input_shape=(10, 5, 1)))
```

有状态的 LSTM 不会在每个批处理结束时重置内部状态。相反，您可以通过调用 reset states（）函数对何时重置内部状态进行细粒度控制。例如，我们可能希望在每个纪元结束时重置内部状态，我们可以执行以下操作：

```python
for i in range(1000):
	model.fit(X, y, epochs=1, batch_input_shape=(10, 5, 1))
	model.reset_states()
```

在进行预测时，还必须使用有状态 LSTM 定义中使用的相同批处理大小。

```python
predictions = model.predict(X, batch_size=10)
```

在评估网络和进行预测时，LSTM 层的内部状态也会累积。因此，如果使用的是有状态的 LSTM，则必须在验证数据集上评估网络或进行预测后重置状态。

默认情况下，一个 epoch 内的样本被 shuffle。当使用多层感知器神经网络时，这是一个很好的实践。如果尝试跨样本保留状态，则训练数据集中样本的顺序可能很重要，必须保留。这可以通过将 fit（）函数中的 shuffle 参数设置为 False 来完成。例如：

```python
for i in range(1000):
	model.fit(X, y, epochs=1, shuffle=False, batch_input_shape=(10, 5, 1))
	model.reset_states()
```

为了更具体地说明这一点，下面是管理状态的三个常见示例：

1. 在每个序列的末尾进行预测，并且序列是独立的。通过将批大小设置为 1，应在每个序列后重置状态。
2. 一个长序列被分成多个子序列（每个序列有多个时间步）。通过使 LSTM 状态化、关闭子序列的洗牌并在每个纪元之后重置状态，应在网络暴露于整个序列之后重置状态。
3. 一个很长的序列被分成多个子序列（每个序列有多个时间步）。训练效率比长期内在状态的影响更为重要，采用 128 个样本的批量，然后更新网络权值并重置状态。

我鼓励您集思广益，找出序列预测问题和网络配置的不同框架，测试并选择那些在预测误差方面最有前途的模型。

## Examples of Preparing Data

很难理解如何准备序列数据以输入 LSTM 模型。对于如何定义 LSTM 模型的输入层，经常会出现混淆。对于如何将序列数据（可能是 1D 或 2D 数字矩阵）转换为 LSTM 输入层所需的 3D 格式，也存在一些混淆。在本节中，您将学习重塑序列数据和定义 LSTM 模型的输入层的两个示例。

### Example of LSTM With Single Input Sample

Consider the case where you have one sequence of multiple time steps and one feature. For example, this could be a sequence of 10 values:

```shell
0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0
```

We can define this sequence of numbers as a NumPy array.

```python
from numpy import array
data = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])
```

We can then use the reshape() function on the NumPy array to reshape this one-dimensional array into a three-dimensional array with 1 sample, 10 time steps and 1 feature at each time step. The reshape() function when called on an array takes one argument which is a tuple defining the new shape of the array. We cannot pass in any tuple of numbers, the reshape must evenly reorganize the data in the array.

```python
from numpy import array
data = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])
data = data.reshape((1, 10, 1))
print(data.shape)
```

```shell
(1, 10, 1)
```

This data is now ready to be used as input (X) to the LSTM with an input shape of (10,1).

```python
model = Sequential()
model.add(LSTM(32, input_shape=(10, 1)))
...
```

### Example of LSTM With Multiple Input Features

Consider the case where you have multiple parallel series as input for your model. For example, this could be two parallel series of 10 values:

```shell
series 1: 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0
series 2: 1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1
```

We can define these data as a matrix of 2 columns with 10 rows:

```python
from numpy import array
data = array([
[0.1, 1.0],
[0.2, 0.9],
[0.3, 0.8],
[0.4, 0.7],
[0.5, 0.6],
[0.6, 0.5],
[0.7, 0.4],
[0.8, 0.3],
[0.9, 0.2],
[1.0, 0.1]])
data = data.reshape(1, 10, 2)
print(data.shape)

```

```shell
(1, 10, 2)
```

This data is now ready to be used as input (X) to the LSTM with an input shape of (10,2).

```pytho
model = Sequential()
model.add(LSTM(32, input_shape=(10, 2)))
...
```

### Tips for LSTM Input

1. This section lists some final tips to help you when preparing your input data for LSTMs.
2. The LSTM input layer must be 3D.
3. The meaning of the 3 input dimensions are: samples, time steps and features.
4. The LSTM input layer is defined by the input shape argument on the first hidden layer.
5. The input shape argument takes a tuple of two values that define the number of time steps and features.
6. The number of samples is assumed to be 1 or more.
7. The reshape() function on NumPy arrays can be used to reshape your 1D or 2D data to be 3D.
8. The reshape() function takes a tuple as an argument that defines the new shape.

## Further Reading

This section provides some resources for further reading.

### Keras APIs

1. Keras API for Sequential Models.
   <https://keras.io/models/sequential/>
2. Keras API for LSTM Layers.
   <https://keras.io/layers/recurrent/#lstm>
3. Keras API for optimization algorithms.
   <https://keras.io/optimizers/>
4. Keras API for loss functions.
   <https://keras.io/losses/>

### Other APIs

1. NumPy reshape() API.
   <https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html>
2. NumPy argmax() API.
   <https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html>
